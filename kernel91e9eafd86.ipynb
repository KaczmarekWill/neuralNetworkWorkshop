{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Neural Network from Scratch\n",
    "\n",
    "We will be using Python 3 along with NumPy (a linear algebra package)  to create a neural network for handwritten digit recognition using the MNIST dataset. \n",
    "\n",
    "Consider writing a computer program to recognize digits. While we may have some intuitions about the general shape of each symbol, the variation in handwriting samples makes the task very difficult to achieve algorithmically. \n",
    "\n",
    "Handwriting recognition is a good tool to introduce neural networks, as it is a relatively straightforward problem for a neural net to solve, and does not require tremendous computational power.\n",
    "\n",
    "## The Perceptron\n",
    "\n",
    "While the sigmoid neuron is the typical neuron model used in neural networks, it will help to first understand a simpler model -- the perceptron.\n",
    "\n",
    "* A perceptron takes several **binary inputs** and produces a single binary output ((draw sample perceptron with x1, x2, x3))\n",
    "* A perceptron uses a set of **weights** on the inputs to determine the values fed in to the perceptron\n",
    "* A perceptron has a **threshold** that determines the minimum value that will cause the perceptron to output 1\n",
    "* This threshold can also be expressed as a **bias**, which is the negative threshold\n",
    "\n",
    "See the code below for an example of how a perceptron works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perceptron(inputs, weights, bias):\n",
    "    total = 0\n",
    "    \n",
    "    for single_input, paired_weight in zip(inputs, weights):\n",
    "        total += single_input * paired_weight\n",
    "        \n",
    "    if total + bias > 0: # if total > threshold\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Neurons\n",
    "\n",
    "We want to devise a system where a small change in a weight will only cause a small corresponding change in the output. Using a perceptron, a small change in the input can potentially cause the output of a perceptron to flip from 0 to 1 or vice versa.\n",
    "\n",
    "Just like the perceptron, the sigmoid neuron has **inputs**, **weights**, and an overall **bias**. But the output is a number between 0 and 1, expressed as σ(wx + b), where σ is called the sigmoid function, and is defined as follows:\n",
    "\n",
    "        σ(z) = 1 / (1 + e^-z)\n",
    "\n",
    "Although we won't go in to detail here, the sigmoid function is helpful in that it is relatively easy to calculate the derivative which is needed for training the model. In fact, your activation function **must** be differentiable in order to perform gradient descent.\n",
    "\n",
    "See an example of a sigmoid neuron below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid_function(weighted_sum):\n",
    "    return 1 / (1 + exp(-weighted_sum)) # exp(x) performs e^x\n",
    "\n",
    "def calculate_sigmoid_neuron(inputs, weights, bias):\n",
    "    total = 0\n",
    "    \n",
    "    for single_input, paired_weight in zip(inputs, weights):\n",
    "        total += single_input * paired_weight\n",
    "        \n",
    "    return sigmoid_function(total + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture\n",
    "\n",
    "* The leftmost layer is known as the **input** layer\n",
    "* The rightmost layer is known as the **output** layer\n",
    "* The middle layers are called **hidden** layers\n",
    "    * Hidden here simply means 'not an input or an output'\n",
    "    \n",
    "Such networks are sometimes called *multilayer perceptrons* or MLPs, despite being made up of sigmoid neurons, not perceptrons.\n",
    "\n",
    "For our handwritten digit network using 28 x 28 images, how many input neurons are there? How many output neurons?\n",
    "\n",
    "The design of input and output layers is relatively straightforward, but there is no simple way to determine the architecture of the hidden layers. There are many ways to make such decisions, but are outside the scope of this lesson\n",
    "\n",
    "We will be using a network called a *feedforward neural network* where information is always fed forward.\n",
    "\n",
    "There are also models which allow feedback loops, which is called a recurrent neural network.\n",
    "\n",
    "Let's take a look at how we will generate a network below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.36694371],\n",
      "       [1.46929057],\n",
      "       [2.16794526]]), array([[-0.73983206]])]\n",
      "[array([[ 0.90551488, -0.08059904,  1.12724755],\n",
      "       [-0.06150455,  0.56386351, -0.43554712]]), array([[ 0.9257386 ],\n",
      "       [ 0.64996471],\n",
      "       [-0.96203509]])]\n"
     ]
    }
   ],
   "source": [
    "#Note that in practice, we will implement this using a class object\n",
    "\n",
    "#np.random.randn(m, n) creates an m x n numpy matrix of \n",
    "#random numbers with mean 0 and variance 1\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def network(sizes):\n",
    "    '''The list 'sizes' contains the number of neurons in each\n",
    "    layer of the network. For example [2, 3, 1] would describe\n",
    "    a network with 2 input neurons, 3 neurons in the hidden\n",
    "    layer, and 1 output neuron'''\n",
    "    num_layers = len(sizes)\n",
    "    \n",
    "    '''Biases are initialized randomly. The input layer does\n",
    "    not need to include a bias so it will be omitted'''\n",
    "    biases = [np.random.randn(x, 1) for x in sizes[1:]]\n",
    "    \n",
    "    print(biases)\n",
    "    \n",
    "    '''Describes a set of weights for the connections between\n",
    "    layers. In the [2, 3, 1] example above, it would create a\n",
    "    2x3 set of weights followed by a 3x1 set of weights.'''\n",
    "    weights = [np.random.randn(x, y) \n",
    "               for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "    \n",
    "    print(weights)\n",
    "    \n",
    "# Example\n",
    "network([2, 3, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning with Gradient Descent\n",
    "\n",
    "* Given x is a 784-dimensional vector (why is this?)\n",
    "* Given y is a 10-dimensional vector\n",
    "* Given a is the output of the network when x is input\n",
    "* Given ||x|| is the length of vector x\n",
    "\n",
    "We would like an algorithm which lets us find weights and biases so\n",
    "that the output from the network approximates y(x) for all training\n",
    "inputs x. To quantify how well we are doing this we define a\n",
    "*cost function*\n",
    "\n",
    "C(w, b) = (1/2n) ∑||y(x) - a||^2\n",
    "\n",
    "C thus represents the **mean squared error** (MSE)\n",
    "\n",
    "As C(w, b) approaches 0, y(x) becomes approximately equal to output\n",
    "a for all training inputs x. So we want to find a set of weights\n",
    "and biases that minimize the cost C(w, b). We will do this using an\n",
    "algorithm called **gradient descent**.\n",
    "\n",
    "We start by thinking of our function as a kind of a valley, and imagining a ball rolling down the slope of the valley. Eventually, the ball will roll to the bottom of the valley. By **computing the derivative** of the shape of the valley, we can **determine the slope** of the valley and therefore which way the ball should roll.\n",
    "\n",
    "### Intuition\n",
    "\n",
    "Consider a model with 2 parameters, v1 and v2. C(v1, v2) can then be represented as a 3d surface just as f(x) can be represented as a 2d line.\n",
    "\n",
    "Think of what happens when we move the ball a small amount Δv1 in the v1 direction, and a small amount Δv2 in the v2 direction. Calculus tells us that C changes as follows:\n",
    "\n",
    "ΔC ≈ (∂C/∂v1)Δv1 + (∂C/∂v2)Δv2\n",
    "\n",
    "We will make ΔC negative, so the ball rolls downhill, and denote the gradient vector by ∇C:\n",
    "\n",
    "∇C = (∂C/∂v1,∂C/∂v2)T\n",
    "\n",
    "We can now rewrite the change ΔC in terms of ∇C and Δv:\n",
    "\n",
    "ΔC = ∇C ⋅ Δv\n",
    "\n",
    "Finally, we can choose a Δv that makes ΔC negative:\n",
    "\n",
    "Δv = −η∇C\n",
    "\n",
    "where η is a small, positive parameter known as the learning rate\n",
    "\n",
    "**Substituting for the formula above, we now have:**\n",
    "\n",
    "ΔC ≈ −η∇C⋅∇C = −η∥∇C∥^2\n",
    "\n",
    "This guarantees that ΔC is negative.\n",
    "\n",
    "This gives us our update rule:\n",
    "\n",
    "v → v′ = v − η∇C\n",
    "\n",
    "We will use this update rule over and over, making moves and decreasing C until we reach a global minimum.\n",
    "\n",
    "### Backpropagation\n",
    "\n",
    "Whereas the error at the output layer is clear, the error at the hidden layers seems mysterious because the training data do not say what value the hidden nodes should have. Fortunately, it turns out that we can **back-propagate** the error from the output layer to the hidden layers. The back-propagation process emerges directly from a derivation of the overall error gradient.\n",
    "\n",
    "At the output layer, we can define the update rule as follows:\n",
    "\n",
    "Δk = Err(k) * sigmoid'(input(k))\n",
    "\n",
    "We can then propagate backwards, defining the update rule as:\n",
    "\n",
    "Δj = sigmoid'(input(k)) ** ∑(weights(j,k) * Δk)\n",
    "\n",
    "\n",
    "### Note\n",
    "\n",
    "We will be using a process called **stochastic gradient descent**, which speeds up the training process by computing the gradient for only a small sample of randomly chosen training inputs at each step. By averaging over many samples we can get a good estimate of the true gradient while speeding up training substantially.\n",
    "\n",
    "This small sample will be referred to as a **mini batch**.\n",
    "\n",
    "## Implementing Our Network\n",
    "If you haven't downloaded the MNIST data, you can get it like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'git' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/mnielsen/neural-networks-and-deep-learning.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: omit the '!' if not using Jupyter Notebook\n",
    "                -or-\n",
    "Download the dataset here:\n",
    "https://github.com/mnielsen/neural-networks-and-deep-learning/archive/master.zip\n",
    "\n",
    "### MNIST Loader\n",
    "\n",
    "This simply loads the dataset and prepares it for feeding in to the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def load_data():\n",
    "    f = gzip.open('data/mnist.pkl.gz', 'rb')\n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding='iso-8859-1')\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def load_data_wrapper():\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    training_data = list(zip(training_inputs, training_results))\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    validation_data = list(zip(validation_inputs, va_d[1]))\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = list(zip(test_inputs, te_d[1]))\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def vectorized_result(j):\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Network\n",
    "\n",
    "This code puts together everything we have worked on today and implements it as a class we can create instances of in order to generate networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pastebin.com/fJ5cHGt5\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a): # Returns the output of the network with a as the input\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            test_data=None):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic\n",
    "        gradient descent.  The ``training_data`` is a list of tuples\n",
    "        ``(x, y)`` representing the training inputs and the desired\n",
    "        outputs.  The other non-optional parameters are\n",
    "        self-explanatory.  If ``test_data`` is provided then the\n",
    "        network will be evaluated against the test data after each\n",
    "        epoch, and partial progress printed out.  This is useful for\n",
    "        tracking progress, but slows things down substantially.\"\"\"\n",
    "        if test_data: \n",
    "            n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print(\"Epoch {0}: {1} / {2}\".format(\n",
    "                    j, self.evaluate(test_data), n_test))\n",
    "            else:\n",
    "                print(\"Epoch {0} complete\".format(j))\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"Update the network's weights and biases by applying\n",
    "        gradient descent using backpropagation to a single mini batch.\n",
    "        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n",
    "        is the learning rate.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = (activations[-1] - y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in range(2, self.num_layers):\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * \\\n",
    "                sigmoid_prime(zs[-l])\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "# Miscellaneous functions\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 8368 / 10000\n",
      "Epoch 1: 9349 / 10000\n",
      "Epoch 2: 9450 / 10000\n",
      "Epoch 3: 9452 / 10000\n",
      "Epoch 4: 9521 / 10000\n",
      "Epoch 5: 9495 / 10000\n",
      "Epoch 6: 9556 / 10000\n",
      "Epoch 7: 9553 / 10000\n",
      "Epoch 8: 9590 / 10000\n",
      "Epoch 9: 9591 / 10000\n",
      "Epoch 10: 9606 / 10000\n",
      "Epoch 11: 9602 / 10000\n",
      "Epoch 12: 9590 / 10000\n",
      "Epoch 13: 9609 / 10000\n",
      "Epoch 14: 9624 / 10000\n",
      "Epoch 15: 9623 / 10000\n",
      "Epoch 16: 9624 / 10000\n",
      "Epoch 17: 9584 / 10000\n",
      "Epoch 18: 9615 / 10000\n",
      "Epoch 19: 9609 / 10000\n",
      "Epoch 20: 9595 / 10000\n",
      "Epoch 21: 9621 / 10000\n",
      "Epoch 22: 9627 / 10000\n",
      "Epoch 23: 9628 / 10000\n",
      "Epoch 24: 9618 / 10000\n",
      "Epoch 25: 9638 / 10000\n",
      "Epoch 26: 9626 / 10000\n",
      "Epoch 27: 9626 / 10000\n",
      "Epoch 28: 9630 / 10000\n",
      "Epoch 29: 9623 / 10000\n"
     ]
    }
   ],
   "source": [
    "training_data, validation_data, test_data = \\\n",
    "    load_data_wrapper()\n",
    "\n",
    "net = Network([784, 60, 10])\n",
    "\n",
    "net.SGD(training_data, 30, 10, 3.0, test_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
